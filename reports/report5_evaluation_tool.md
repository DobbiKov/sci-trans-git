# Translation evaluation tool

## Introduction

In the automatic translation development tool it is always required to test the quality of translation. Doing such tests manually reading each translation is a very time consuming process. Consequently, the primary goal of this tool is to provide a comprehensive assessment of machine-translated documents by comparing them against original source documents and, where available, human-translated references. The evaluation spans three main axes: natural language (NL) quality, preservation of document structure, and (for LaTeX) visual layout fidelity.

The tool is a Python-based CLI, configured via `config.py`, and designed to produce both detailed JSON reports and summary CSV files. It leans on external tools like Pandoc for document parsing and several specialized Python libraries for metric calculation.

## Overall Workflow & Architecture

The evaluation pipeline is managed by the main script, `run_evaluation.py`.

1.  **Configuration Loading:** The process begins by loading settings from `config.py`. This is critical as it defines:
    - Paths: `SOURCE_DIR`, `LLM_TRANSLATED_DIR`, `HUMAN_REFERENCE_DIR`, `REPORTS_DIR`, `LOGS_DIR`.
    - Operational Parameters: `LANG_PAIRS` (e.g., `fr` to `en`), `DOC_FORMATS` (e.g., `markdown`, `latex`).
    - Metric Selection: Boolean flags like `RUN_BLEU`, `RUN_AST_COMPARISON`, `RUN_VISUAL_DIFF` control which evaluations are performed.
    - Tool Paths: `PANDOC_PATH`, and rendering specifics like `PDF_DPI`.

2.  **Directory Setup:** The tool ensures all necessary data, report, and log directories (specified in `config.py`) are created if they don't exist.

3.  **Iterative Evaluation Loop:**
    - The `main()` function in `run_evaluation.py` iterates through each language pair and document format defined in `config.py`.
    - `document_parser.get_document_paths()` is used to locate all source documents.
    - For each source document:
        - File paths for the LLM-translated version and (if `HUMAN_REFERENCE_DIR` exists) the human reference are constructed based on a naming convention (e.g., `doc_id_<target_lang>.<ext>`).
        - If the LLM-translated file is found, `evaluate_document_pair()` is invoked.
        - Results from each pair are aggregated.

4.  **Report Generation:**
    - A detailed JSON report (`evaluation_results_YYYYMMDD-HHMMSS.json`) is generated by `utils.save_json_report()`, containing all metrics for every document processed.
    - A summary CSV report (`evaluation_summary_YYYYMMDD-HHMMSS.csv`) is created by `utils.save_csv_report()`, offering a flattened view of key scores (NL metrics, structural issue counts, visual RMSE) for quick comparison.

5.  **Logging:** `utils.log_message()` provides ongoing feedback, warnings, and errors to both the console and a persistent log file (`logs/evaluation.log`).

## Core Evaluation Logic: `evaluate_document_pair`

This function is the engine of the evaluation, processing a single source, its LLM translation, and an optional human reference. It compiles and returns a dictionary of all results for that document.

### Natural Language (NL) Translation Quality

- **Text Extraction:**
    - The `document_parser.extract_natural_language_text()` module is responsible for isolating plain text.
    - For LaTeX: It uses `pypandoc` for an initial conversion to 'plain' text, followed by a series of regular expressions to strip LaTeX commands, environments, math, and comments.
    - For Markdown/MyST: Similar to LaTeX, `pypandoc` converts to 'plain' text, then regex cleans up Markdown syntax (headings, emphasis, links).
    - If text extraction fails (e.g., file not found, pandoc error), an error is logged, and subsequent NL metrics might be skipped.
- **Metric Calculation (via `nl_evaluator.py`):**
    - These metrics are computed only if their respective `RUN_*` flags in `config.py` are `True` AND a human reference text is successfully extracted.
    -  **BLEU:** `nl_evaluator.calculate_bleu()` uses `sacrebleu.sentence_bleu()`.
    -  **TER:** `nl_evaluator.calculate_ter()` uses `sacrebleu.sentence_ter()`.
    -  **BERTScore:** `nl_evaluator.calculate_bert_score()` leverages the `bert_score` library, reporting the F1.
    -  **COMET:** `nl_evaluator.calculate_comet()` uses the `comet` library (model `Unbabel/wmt22-comet-da` is downloaded if needed).
- **Issues & Skippings:**
    - NL metrics are skipped if no human reference is provided or if text extraction from the reference/LLM output yields empty content.

### Structural Preservation Checks

These checks assess whether the document's non-textual framework and specific non-translatable elements are maintained.

- **Pandoc AST (Abstract Syntax Tree) Generation:**
    - `document_parser.get_pandoc_ast()` converts source and LLM-translated documents into Pandoc's JSON AST format using `pypandoc`. Media files (e.g., images) are extracted during this process.
- **Special Block Extraction:**
    - `document_parser.extract_special_blocks()` directly parses raw document files (LaTeX, Markdown/MyST) using regular expressions to identify and extract:
        - `code_blocks` (e.g., `\begin{lstlisting}`, ```...```)
        - `equations` (e.g., `$$...$$`, `$...$`)
        - `image_paths`
        - `labels` (e.g., `\label{key}`)
        - `references` (e.g., `\ref{key}`)
- **Metric Calculation (via `struct_evaluator.py`):**
    - These are run if their `RUN_*` flags are `True` and ASTs/special blocks are available.
    - **AST Comparison (`RUN_AST_COMPARISON`):**
        - `struct_evaluator.compare_pandoc_asts()` performs a recursive comparison of the source and LLM-translated ASTs.
        - It flags differences like:
            - `extra_node`/`missing_node`
            - `type_mismatch` (e.g., `Para` vs `Header`)
            - Content mismatches for nodes that should be identical (`Code`, `RawBlock`, `Image` path, `Math` markup, `Link` target, `Citation` key).
            - `missing_child`/`extra_child`
            - Attribute differences (`header_level_mismatch`, `list_type_mismatch`).
        - Outputs include a list of differences and a total count.
    -  **Special Block Preservation (`RUN_PATTERN_MATCHING`):**
        - `struct_evaluator.check_special_block_preservation()` compares the `special_blocks` from source and LLM documents.
        - Looks for mismatches in count and content of `code_blocks` and `equations`.
        - Checks if the set of `image_paths` remains consistent.
    - **Reference Integrity (`RUN_REF_INTEGRITY`):**
        - `struct_evaluator.check_reference_integrity()` analyzes `labels` and `references` from `special_blocks`.
        - Identifies issues: `missing_labels`, `extra_labels`, `reference_count_mismatch`, `reference_key_mismatch`, `missing_reference_in_target`, `dangling_reference`.
- **Issues & Skippings:**
    - AST-based checks are skipped if Pandoc AST generation fails.

### Visual/Layout Comparison

Currently, this is **exclusively for LaTeX documents** (`doc_format == 'latex'`) and active only if `config.RUN_VISUAL_DIFF` is `True`.

- **Workflow (via `rendering_evaluator.py` - *Note: The provided file content for `rendering_evaluator.py` appears to be a duplicate of `struct_evaluator.py`. The description below is based on its intended functionality as inferred from `run_evaluation.py`.*):**
    1. **LaTeX to PDF Compilation:** `rendering_evaluator.compile_latex_to_pdf()` (inferred) compiles both source and LLM-translated LaTeX files into PDFs. These are likely stored in `reports/<doc_id>/`.
    2. **PDF to Image Conversion:** `rendering_evaluator.convert_pdf_to_images()` (inferred) converts PDF pages into raster images (e.g., PNGs) at `config.PDF_DPI`, also stored in the report subdirectory.
    3. **Image Comparison:** `rendering_evaluator.compare_images_visually()` (inferred) compares corresponding image pages from source and LLM outputs, calculating Root Mean Square Error (RMSE) and generating visual diff images (`*_diff.png`).
- **Metrics Reported:**
    - `page_count_source`, `page_count_llm`.
    - `page_rmse`: A list, each with page number, RMSE, and path to its diff image.
    - `average_rmse`: Mean RMSE across all compared pages.
- **Issues & Skippings:**
    - Skipped if not LaTeX or `RUN_VISUAL_DIFF` is `False`.
    - Skipped if PDF compilation or image conversion steps fail.

## Key Modules and Responsibilities

- **`run_evaluation.py`**: The main orchestrator of the evaluation pipeline.
- **`config.py`**: Central hub for all configurations, paths, and flags.
- **`src/document_parser.py`**:
    - Locates document files.
    - Extracts plain natural language text using Pandoc and regex.
    - Generates Pandoc ASTs (JSON).
    - Extracts "special blocks" (code, equations, image paths, labels, references) via regex from raw files.
- **`src/nl_evaluator.py`**:
    - Calculates NL quality metrics: BLEU, TER, BERTScore, COMET.
- **`src/struct_evaluator.py`**:
    - Compares Pandoc ASTs for structural deviations.
    - Checks the preservation of "special blocks."
    - Verifies reference integrity (labels and citations).
- **`src/rendering_evaluator.py`**:
    - *(Inferred functionality due to file content issue)* Manages LaTeX-to-PDF compilation, PDF-to-image conversion, and visual image comparison (RMSE, diff images).
- **`src/utils.py`**:
    - Provides utility functions for saving JSON/CSV reports and logging messages.

## How the Tool Evaluates Translations

The tool employs a multi-faceted approach:

1.  **NL Quality (Fluency & Adequacy):** Compares LLM output against human reference text.
    - Metrics like **BLEU** and **TER** look at lexical overlap and edit distance.
    - **BERTScore** and **COMET** use contextual embeddings for deeper semantic similarity.

2.  **Structural Fidelity:** Assesses if the document's non-textual scaffolding is preserved.
    - **AST Comparison:** Checks for changes in document hierarchy (paragraphs, headings, lists) and critical inline elements (links, code, math).
    -  **Special Block Preservation:** Ensures non-translatable content like code blocks, equations, and image paths remain untouched.
    -  **Reference Integrity:** Validates that internal cross-references and citations are correctly maintained.

3.  **Visual Layout Preservation (LaTeX only):** Compares the rendered visual output.
    - **RMSE** quantifies pixel-level differences, highlighting changes in spacing, line breaks, image placement, etc.

## Conclusion

This translation evaluation tool provides a robust automatic framework for
assessing machine translation quality, going beyond simple text-to-text
comparisons. Its strengths include its modularity, configurability via
`config.py`, and its tripartite evaluation covering NL quality, structural
integrity, and (for LaTeX) visual layout.
