# Smaller model testing

## Introduction
The choice of the _Large Language Model_ is one of the most important in the
whole process of the development of the document translation tool. This is
important due to the next factors:
- translation quality
- human like kind of the translated text
- preserving structure and layout

However, choosing the best market models for the product is not always the best
option, especially when the privacy is in high priority as well as energy
consumption concerns.

This leads us to the essence of exploring and testing different models for our
purposes.

## Models choice
For the current version of the tool (date: 10/06/2025), the google's
`gemini-2.0-flash` is used mostly. The model provides satisfying results while
the usage cost is acceptable. Moreover, the energy consumption of such model is
the lowest on the market. The main concern is privacy.

For this report the next model are chosen:
- `llama-3.3-70B` - open source model from Meta that is hosted and provided by
  the Paris-Saclay University that is the main reason to choose such a model.
- `gemma3-27B` - open weights model from google that is more lightweight model
then `llama-3.3-70B` mentioned above still providing almost the same results.
Main reasons to choose this model is the ease-of-use as it's provided freely 
by the google gen-ai API. The energy consumption of such model is comparable
to the llama one. The model can be hosted on a local environment, such as 
powerful laptop or a local data center.

## Testing methods
Code translation has been already tested in the
[previous](./report3_test_aristote.md) report, thus we can conclude that
smaller models aren't still usable for the machine translation in the code.

Thus, the plain text translation is tested.

The data for testing has been taken from the SageMath book and Paris-Saclay
[astronomy course](https://m2-npac-ac.pages.in2p3.fr/). For the translation the
data has been taken by paragraphs mostly without any non-text elements such as
LaTeX formules or markdown syntax elements.

The two models has been asked to translate the given texts from French To
English and from English to French with a simple zero-shot prompting technique.

## Evaluation methods
For the evaluation purposes an appropriate
[tool](https://github.com/DobbiKov/translation-evaluator) has been developed
that takes
source document, machine translation document and human reference and returns
the result using well-known metrics such as _BERT_ ([1]), _BLEU_([2]), _TER_([3]).

However, the automatic evaluation is not always as reliable, thus the text has
been read by humans who left their comments on the translation quality.

### About metrics
- **BLEU** - a very popular metrics to compare translations. It primarily
  measures n-gram overlap between the candidate and references, giving a higher
  score for longer matching sequences. The score is on the scale from 0 to 100
  where more is better.

- **BERT** - BERTScore evaluates text similarity by calculating the cosine
  similarity between contextualized word embeddings (generated by BERT) of the
  candidate translation and the reference translation(s). Unlike n-gram overlap
  metrics like BLEU, BERTScore captures semantic similarity by considering the
  meaning of words in their context, making it a better indicator of fluency
  and adequacy. It computes separate precision, recall, and F1 scores based on
  this similarity. The score is on the scale from 0 to 1 where more is better.

- **TER** - (Translation Edit Rate) is a post-editing metric that measures
  the number of edits required to transform a machine translation into a
  perfect reference translation, normalized by the average number of words in
  the reference.
  Essentially, it calculates the minimum number of word-level operations
  (insertions, deletions, substitutions, and shifts of word sequences) needed
  to correct the machine output. A lower TER score indicates a higher quality
  translation, as fewer edits are needed.


## Results

### Automatic evaluation
#### English to French translation
- `aristote`
    - bleu: 52.46
    - ter: 33.46
    - bert_score: 0.88

- `gemma`
    - bleu: 52.26
    - ter: 35.55
    - bert_score: 0.88


- `gemini`
    - bleu: 58.26
    - ter: 30.6
    - bert_score: 0.87

#### French to English translation
- `aristote`
    - bleu: 54.50
    - ter: 33.00
    - bert_score: 0.94

- `gemma`
    - bleu: 56.10
    - ter: 31.88
    - bert_score: 0.94

- `gemini`
    - bleu: 62.10
    - ter: 27.53
    - bert_score: 0.94

#### Conclusions
- The 52 BLEU score an higher means the translation is on near human level. Above 60: human like translation.
- <35 TER score is considered as _good_ to _very good_ and means that the MT followed with post-editing is more efficient that only human translation.
- >85 BERT score is considered as _STRONG_ translation and >92 as Near human translation.

The above results lead us to the conclusion that all the three model excel in
machine translation and give very strong translation results.
However, it is easy to notice that in all the cases gemini provide better
results then the other two models.

### Human evaluation
After the human evaluation the next points have been explored:
- smaller models (here: `aristote` and `gemma`) tend to remove non-text
  elements such as \*markdown emphasizing\* or latex formulas if such
  presented.
- smaller models provide far worse results when translating to not world wide
  languages such as Ukrainian whereas `gemini` doesn't change its results. 
  However, worth noticing that `gemma` model provides better translation for
  Ukrainian language that `aristote`.


## Conclusions
`llama-3.3-70B` and `gemma-3-27B` provide strong translation for world-wide
languages such as French and English and worse translation quality for less
popular languages such as Ukrainian.

The models are usable and excel in plain-text only translation and unreliable 
when the structure and syntax preserving is required.

`gemini-2.0-flash` model provides better results then the other two models 
but the difference is not as big when translation plain-text in popular languages. 


# Literature
- [1: BERT score](https://huggingface.co/spaces/evaluate-metric/bertscore)
- [2: BLEU score](https://huggingface.co/spaces/evaluate-metric/bleu)
- [3: TER score](https://huggingface.co/spaces/evaluate-metric/ter)
